\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../../Comments}
\input{../../Common}
%\newcommand{\li}[1]{\texttt{#1}}
\begin{document}

\title{A Fourier Series Library: System Verification and Validation Plan for FSL} 
\author{Bo Cao}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Oct. 28, 2019  & 0.99 & First draft.\\
Dec. 5, 2019 & 1.0 & Final version.\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\newpage

\section{Symbols, Abbreviations and Acronyms}

Some symbols, abbreviations and acronyms are defined in the Software Requirements Specification(SRS)
document\footnote{This document is available at
\url{https://github.com/caobo1994/FourierSeries/blob/master/docs/SRS/SRS.pdf}.}. For
simplicity and maintainability, they are not redefined here. Readers shall refer
to the CA documents when a certain item is not defined here.

\vspace{1cm}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}


\section{General Information} This document provides an overview of the
Verification and Validation (VnV) plan for the Fourier Series Library (FSL). It
lays out the purpose, methods, and test cases for the VnV procedure.

\subsection{Summary}

The library to be tested is called the Fourier Series Library (FSL). This
library performs a set of computations, transformations, and/or input/output at
the request of the library user.

\subsection{Objectives}

The intended objective of the VnV procedure is to verify that this library has
generally met the requirements described in the CA document. These requirements
include the functional requirements (FRs) and the non-functional requirements
(NFRs).

Note that if a small part of the NFRs has not been met, the library is still
acceptable when the not-met NFRs' impact has been analyzed and deemed
non-essential.

\subsection{Relevant Documentation}

As we said before, this document relies on the CA document. This document is
also the base of the Unit Test Plan document.


\section{Plan}
This section lists the plan of the VnV of the FSL library.
\begin{itemize}
	\item \autoref{sbsc:VnVTeam} introduces the Validation and Verification Team of this library, including the team members and their duties.
	\item \autoref{sbsc:SRSVnVPlan} outlines the plan for the verification of the SRS.
	\item \autoref{sbsc:DesignVnVPlan} outlines the plan for the verification of the design of this libary.
	\item \autoref{sbsc:ImplementationVnVPlan} outlines the plan for the verification of the library's implementation.
	\item \autoref{sbsc:SoftwareVnVPlan} outlines the validation plan of the library based on the pseudo-oracle provided by MATLAB.
\end{itemize}
\wss{There should usually be text between section headings.  In this case, a
  ``roadmap'' of the subsections in this section would be appropriate.}
\an{Contents added here}

\subsection{Verification and Validation Team}\label{sbsc:VnVTeam}

The major member of the team is the author himself. Other contributors might
assist in the VnV procedure, but their contributions are not
guaranteed.
In detail, the other contributors include the following people and their github accounts.
\begin{itemize}
	\item Dr.~Spencer Smith \href{https://github.com/smiths}{smiths} and Deema Alomair \href{https://github.com/deemaalomair1}{deemaalmair1} review the whole library, including all documents and codes.
	\item Ao Dong \href{https://github.com/Ao99}{Ao99} reviews the SRS.
	\item Peter Michalski \href{https://github.com/peter-michalski}{peter-michalski} reviews the VnV plans.
	\item Sasha Soraine \href{https://github.com/sorainsm}{sorainsm} reviews the design.
\end{itemize}

\wss{You should specifically list the class mates that are assigned
  to review your documents.  You should also list the course instructor.}\an{Done.}

\subsection{SRS Verification Plan}
\label{sbsc:SRSVnVPlan}

The verification of the SRS document mainly consists of the checking of all the mathematical expressions of the system. Especially, we will use some simple mathematical functions and their related results to check the theories related to operations.

Some functions are reversions of other functions. For example, \li{Function Value} is the reversion of \li{Transform}, \li{Addition} is the reversion of \li{Subtraction}, \li{Division} is the reversion of \li{Multiplication}. For each pair of function \li{A} and \li{B}, we will see if functions \li{A(B())} and \li{B(A())}'s output is close to its input with some examples. 

We will also learn from the feedback from
reviewers, and the author's experience in developing and verifying this
library. \wss{A task directed verification plan could be much more effective
  than this ad hoc approach.}\an{Some ways added before the original content.}

\subsection{Design Verification Plan}
\label{sbsc:DesignVnVPlan}

The design of this library will be verified by checking the library against all the requirements. The functional requirements shall be checked first, followed by the performance-related parts of the non-functional requirements. \wss{This is not
  a very detailed plan.  Did you consider other options?  Some of your
  colleagues, like Peter, have a detailed plan for design verification.}\an{Amended.}

\subsection{Implementation Verification Plan}
\label{sbsc:ImplementationVnVPlan}

The verification of the implementation of this library is mainly done by unit
testing. The detail of unit testing can be found in the Unit Test Plan
document. Mainly, the unit test will be done by first testing the basic
functions in this library, and then testing the advanced functions. \textbf{Please note
that the test result of any function in this library is acceptable, if and only
if the reliant functions of this function is tested to be right.}

\subsection{Software Validation Plan}\label{sbsc:SoftwareVnVPlan}

The transformation part of this library will be validated by comparing its
result with the MATLAB pseudo-oracle. 

The MATLAB pseudo-oracle is computed by the implemented Fourier Transformation function within the MATLAB math libraries. We will choose several groups of $f(t), \omega, n$, compute the Fourier Transformation of $f(t)$ with frequency $\omega$, and compare the leading terms with $\mathit{CFS}(f(t), n, \omega)$  \wss{Where does the Matlab pseudo-oracle
  come from?  This is a good idea, but you should take it a step further and
  explain the source of the pseudo-oracle.}\an{Elaborated here.}

\section{System Test Description}
	
\subsection{Tests for Functional Requirements} All tests in this section will be
done by unit testing, the detail of which will be covered in the Unit Test VnV
Plan document.

Since these tests involve comparing floating point numbers, and there are
intrinsic errors regarding floating point computations, we have to set a
tolerance, so that two floating numbers with difference smaller than this
tolerance are considered equal. We use $\epsilon$ to represent this tolerance,
and choose its value as $10^{-6}$ for all tests. This value is chosen because it
is large enough to include all computation-related error, but small enough to
detect any error in the result of the library. This value can be changed if we
have good reasons to support this change.  \wss{Yes, using a symbolic constant
  for this is a good idea. I suggest that you put this constant in a table in
  the Appendix to this report, so that it is easy to maintain.  I see that you
  are varying $\epsilon$ throughout this document.  You should say this here,
  and not follow my advice to add a table of constants at the end, since the
  value is not constant.}

\subsubsection{Module 1: Basic comparison function}

The tests here are selected to cover the tolerated comparison function and its
base, subtraction operation and amplitude function. The tolerated comparison
function will be used in later tests to compare the resulted CFS of the tested
function with its (pseudo-)oracle counterpart, so we need to test this function
first to ensure that the following test results are reliable.

\textbf{NOTE: Do not proceed with other modules unless you have succeed in this
module. For each round of test, the result of other modules is trustworthy if
and only if the test results of this module are all successes.}

\begin{enumerate}
	
	\item{Test of subtraction: \\}
	
	Type: Automatic
	
	Initial State: The subtract function of the loaded FSL library.
	
	Input: $\mathit{CFSf}=[n=2, \omega=1.0, A=\{1.0, 0.0, 0.0\}, B=\{0.0, 1.0\}]$ and 
	$\mathit{CFSg}=[n=2, \omega=1.0, A=\{0.0, 2.0, 1.0\}, B=\{1.0, 0.0\}]$	 
	
	Output: Evaluated result of $\mathit{CFSf}-\mathit{CFSg}$, which should
        be a CFS object
        $[n=2, \omega=1.0, A=\{1.0, -2.0, -1.0\}, B=\{-1.0, 1.0\}]$
	
	Test Case Derivation: Feed input, get output, and compare value-to-value automatically via Unit Test. \wss{Why do this manually?
          This is a good test for automation.  A unit testing framework can take
        care of the details.  I see that many of the following tests are
        manual.  Please consider making all of your tests automatic.}\an{I misunderstood the meaning of 'Test Case Derivation'. I thought it means how I find a set of input/outputs. Now, I found out that it means how we compare our output with the standard output.}
	
	How test will be performed: Unit Test framework will feed function with the aforementioned input,
        and compare the function output to the aforementioned standard output by
        comparing its $n$, $\omega$, $A_i$'s and $B_i$'s variable-by-variable. Called 'compute and compare later'.
	
	\item{Test of amplitude function: \\}					
	
	Type: Automatic
	
	Initial State: None.
	
	Input: $\mathit{CFSf}=[n=2, \omega=1.0, A=\{1.0, 2.0, 2.0\}, B=\{2.0,
        2.0\}]$ and $\epsilon=10^{-6}$.
	
	Output: Evaluated result of
        $|\mathit{Amp}(\mathit{CFSf})-3.0|\leq\epsilon$, which should be
        \li{True}.
	
	Test Case Derivation: Feed input, get output, and compare value-to-value automatically via Unit Test.
	
	How test will be performed: Compute and compare.
	
	
	\item{Test of tolerated equality function \li{TolEq} (\li{True} result): \\}
	
	Type: Automatic
	
	Initial State: Verified amplitude function.
	
	Input: $\mathit{CFSf}=[n=2, \omega=1.0, A=\{1.0, 0.0, 0.0\}, B=\{0.0, 1.0\}]$, 
	$\mathit{CFSg}=[n=2, \omega=1.0, A=\{0.0, 2.0, 1.0\}, B=\{1.0, 0.0\}]$, and error $\epsilon=10.0$.
	
	Output: Return value of this function, which should be \li{True} \wss{What is being
          compared?  This is not clear.}\an{I mean the return value of this function}
	
	Test Case Derivation: Feed input, get output, and compare value-to-value automatically via Unit Test.
	
	How test will be performed: Compute and compare.
	
	\item{Test of tolerated comparison function \li{TolEq} (\li{False} result):\\}
	
          Same as \textit{Test of tolerated comparison (\li{True} result)}, but
          with $\epsilon=1.0$ and Output as \li{False}.
\end{enumerate}

\subsubsection{Module 2: Fourier transformation and approximation}
This module tests the functions that compute Fourier transformation and
approximated values of functions.

\begin{enumerate}
	\item{Test of coefficient (even function):\\}
	
	Type: Automatic
	
	Initial State: Verified tolerated equality function \li{TolEq}.
	
	Input: $f(t)=t^2$, $\omega=1$, $n=2$, $\epsilon=10^{-6}$,
        $\mathit{CFSstd}=[n=2,\omega=1, A=\{\pi^2/3, -4.0, 1.0\}, B=\{0.0,
        0.0\}]$
	
	Output: Evaluated result of
        $\mathit{TolEq}(\mathit{CFSf}, \mathit{CFSstd}, \epsilon)$, which should
        be \li{True}. \wss{Is this TolEq function defined somewhere?}\an{In SRS}
	
	Test Case Derivation: Feed input, get output, and compare with \li{TolEq} and $\epsilon$ automatically via Unit Test.
	
	How test will be performed: Compute and compare, but instead of compare one-by-one, use \li{TolEq} function with the aforementioned tolerance $\epsilon$. (Called 'Compute and compare with tolerance' later.)
	
	\item{Test of coefficient (odd function):\\}
	
	Type: Automatic	
	
	Initial State: Verified tolerated equality function.
	
	Input: $f(t)=t$, $\omega=1$, $n=2$, $\epsilon=10^{-6}$,
        $\mathit{CFSstd}=[n=2,\omega=1, A=\{0.0, 0.0, 0.0\}, B=\{-2.0, 1.0\}]$
	
	Output: $\mathit{TolEq}(\mathit{CFSf}, \mathit{CFSstd}, \epsilon)$,
        which should be \li{True}.
	
	Test Case Derivation: Feed input, get output, and compare by \li{TolEq} and $\epsilon$ automatically via Unit Test.
	
	
	How test will be performed: Compute and compare with tolerance.
	
	\item{Test of approximated function value: \\}
	
	Type: Automatic
	
	Initial State: None.
	
	Input: $\mathit{CFSf}=[n=2,\omega=1, A=\{0.0, 2.0, 0.0\}, B=\{-2.0,
        1.0\}]$, $t=\pi/4$, $\epsilon=10^{-6}$.
	
	Output: Evaluated result of
        $|\mathit{App}(\mathit{CFSf}, t)-(2+\sqrt{2}/2)|\leq\epsilon$, which
        should be \li{True}.
	
	Test Case Derivation: Feed input, get output, and compare by \li{TolEq} and $\epsilon$ automatically via Unit Test.
	
	
	How test will be performed: Compute and compare with tolerance.
	
\end{enumerate}

\subsubsection{Module 3: Operations and functions}

\begin{enumerate}
	\item{Test of addition: \\}
	
	Type: Automatic
	
	Initial State: Verified tolerated equality function.
	
	Input: $\mathit{CFSf}=[n=2,\omega=1, A=\{0.0, 2.0, 0.0\}, B=\{-2.0,
        0.0\}]$,
        $\mathit{CFSg}=[n=2,\omega=1, A=\{1.0, 0.0, 2.0\}, B=\{0.0, 1.0\}]$,
        $\mathit{CFSstd}=[n=2,\omega=1, A=\{1.0, 2.0, 1.0\}, B=\{-2.0, 1.0\}]$,
        $\epsilon=10^{-6}$
	
	Output: Evaluated result of
        $\mathit{TolEq}(\mathit{CFSf}+\mathit{CFSg}, \mathit{CFSstd},
        \epsilon)$, which should be \li{True}.
	
	Test Case Derivation: Manual Computation.
	
	How test will be performed: Compute and compare.
	
	\item{Test of multiplication:\\}
	
	Type: Automatic
	
	Initial State: Verified tolerated equality function.
	
	Input: $\mathit{CFSf}=[n=2, \omega=1, A=\{1.0, 0.0, 1.0\}, B=\{1.0, 0.0\}]$,
	$\mathit{CFSg}=[n=2, \omega=1, A=\{1.0, 1.0, 0.0\}, B=\{1.0, 0.0\}]$, 
	$\mathit{CFSstd}=[n=2, \omega=1, A=\{2.0, 2.0, 0.0\}, B=\{2.0, 1.0\}]$,
	$\epsilon=10^{-6}$
	
	Output: Evaluated result of
        $\mathit{TolEq}(\mathit{CFSf}*\mathit{CFSg}, \mathit{CFSstd},
        \epsilon)$, which should be \li{True}.
	
	Test Case Derivation: Manual Computation.
	
	How test will be performed: Compute and compare.
	
	\item{Test of division: \\}
	
          Same as \textit{Test of multiplication}, but swap $\mathit{CFSf}$ and
          $\mathit{CFSstd}$, while change $*$ to $/$.
	
	\item{Test of function of CFS: \\}
	
	Type: Automatic
	
	Initial State: Verified tolerated equality, addition and multiplication function.
	
	Input: $\mathit{CFSf}=[n=2, \omega=1, A=\{1.0, 0.0, 1.0\}, B=\{1.0,
        0.0\}]$, $g(t)=e^t$, and $\epsilon=10^{-6}$.
	
	Output: Evaluated result of
        $\mathit{TolEq}(g(\mathit{CFSf}), 1+\mathit{CFSf}+0.5\mathit{CFSf}^2,
        \epsilon)$, which should be \li{True}.
	
	Test Case Derivation: Manual Computation.
	
	How test will be performed: Compute and compare.
	
\end{enumerate}

\subsubsection{Test of conversion}

\begin{enumerate}
	\item{Test of conversion from other data format:\\}
	
	Type: Automatic
	
	Initial State: Verified tolerated equality function
	
	Input: $n=2$, $\omega=1$, $A={1.0, 0.0, 2.0}$, $B={2.0, 0.0}$.
	
	Output: constructed CFS
	
	Test Case Derivation: None
	
	How the test will be performed: convert and compare the result with a
        standard CFS using tolerated equality and $\epsilon=10^{-6}$.
	
	\item{Test of conversion to other data format:\\}
	
          Same as \textit{Test of conversion from other data format}, while
          exchanging input and output, and compare each coefficients with
          tolerance as $\epsilon$.
	
\end{enumerate}
\subsection{Test for input constraints}
We design input test for detecting mismatch $n$ and $\omega$ for functions that
accept two CFS's as inputs. For each test in the following list, we derive tests
for input constraints based on it.

\begin{itemize}
	\item Test of addition
	\item Test of subtraction
	\item Test of multiplication
	\item Test of division
	\item Test of tolerated equality
\end{itemize}

For each test in this list, we derive two tests. One is to change the $\omega$
in the second CFS to the half of its original value, and the other is to change
the $n$ in the second CFS to $1$, and remove $A_2$ and $B_2$ of the second CFS
accordingly. Error message indicating a pair of mismatched CFS's shall appear.
 
Additionally, we do input constraint check on the tests of conversion from other
data formats. We make $n$ in the original test from 2 to 3, and error message
indicating mismatched $n$, number of $A_i(i\neq0)$'s, and number of $B_i$'s.

\subsection{Tests for Nonfunctional Requirements}
The tests in this section will also be done by unit testing.

\subsubsection{Speed evaluation}
We test the speed of IM2, IM3, IM4, IM5, IM6, IM7, IM8 and IM9.	

For each test, generate random CFS's with same $\omega$ and various $n$, as well as $A_i$'s and $B_i$'s from the same random number generator. \wss{be
  more specific on what is involved in generating the random CFS's.  What is the
  range of values that are allowed?}\an{The range of the values (fully decided by the random number generator) is the same within each test case, but may vary among test cases in the future, so I choose not to mention the detail of the range here, but within the test case.} clock
the execution time with the generated CFS's as input, and check whether the
relationship between $n$ and the execution time follows the requirements in the
NFR.

We will demonstrate how the test is performed by showing a template of tests, and each test's values of template parameters (shown in the following font \li{Parameter}).
\begin{enumerate}
	
	\item{Test the speed of \li{Operation}\\}
	
	Type: Automatic
	
	Initial State: \li{Operation}
	
	Input/Condition: Various pair of CFS's, with the same $\omega$,
        $n=100:100:1000$ respectively, and $A_i$'s and $B_i$'s generated from a
        random number generator. In this test, this generator is chosen as the
        uniform random number generator on $[-1.0, 1.0]$.
	
	Output/Result: A plot of average execution time versus $n$, together with the related \li{Speed Rule} regression coefficient $r$ (if the value of \li{Speed Rule} is unknown, we will report the $r$, $a$, and $b$ of the log-log regression $\log(y)=a\log(x)+b$ instead) \wss{I think the output should be the plot of the time
          versus $n$.}
	
	How test will be performed: For each $n$, generate 10 pairs of CFS's,
        clock their execution time to take average value as the execution time
        for CFS's of size $n$, plot as said before, find and show the $r$ value of a \li{Speed Rule} fitting between
        $n$ and the execution time. Usually, this test is considered a success when $|r|>0.9$, but it can also be considered a success if the tester has found a good explanation of this coefficient.
	
\end{enumerate}
Here is a list of the values of parameters in each test.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		IM\#&\li{Operation}&\li{Speed Rule}\\
		\hline
		IM2&Approximate Function Value&Linear\\
		IM3&Addition&Linear\\
		IM4&Subtraction&Linear\\
		IM5&Multiplication&Squared\\
		IM6&Division&Unknown\\
		IM7&Function&Unknown\\
		IM8&Amplitude&Linear\\
		IM9&Tolerated Equality&Linear\\
		\hline
	\end{tabular}
	\caption{Table of parameter values for each test.}
\end{table}

\wss{You should explain all of the tests, but you can use a table to summarize
  just the deltas between the base test and the individual tests.}\an{Done accordingly.}

\wss{What about other nonfunctional tests?  Usability could be a valuable thing
  to assess.  Portability? (I think this would be a fairly easy one to assess.)
  Installability?}
\an{The usability test is deemed unnecessary, due to the fact that the interface of this library is pretty simple, and given my experience of using several libraries, I think that it is quite easy to use if the user have basic knowledge of this language.}
\an{Initially when I think about my library at this stage, I have not decided about the language, test tools, and other things. I only knows that I will distribute the source code. In the case of source code distribution, the common practice among the open-source software area is that the users shall fully taken care of the portability and installability part, thus I did not plan to test them here.} 
	
\an{Furthermore, I anticipate that I will be writing codes strictly following basic standards with popular supporting libraries (like boost), and the portability of these standards and libraries have been tested by lots and lots of people on different kinds of platforms, so the portability of my library is guaranteed.}
	
\an{I will also organize my library in a popular way, and when used by the users, the library will not contain any codes to be compiled when being installed, just some source codes, and the users will compile my source code together with theirs. Thus, the install procedure only involves that the user copy/link my library files to their intended location, which is pretty easy and straightforward on the platforms that I know of, so the installability is guaranteed.}

\an{However, after I have considered the actual unit test part, I think I can do  a little bit to the two test just to provide proof to my arguments. The contents I added is shown as follows.}

\subsubsection{Test of portability and installability}
The portability and installability test will be done by doing the unit tests on several platforms, with different compilers and different versions of language standards.

The platforms include mac OS X (10.11~10.15), Windows 10 (1909 as a representative, since Windows has a good reputation of compatibility between versions), and Linux distributions including Ubuntu 18.04, CentOS 7.7-1908, Fedora 30, and Archlinux 2019-12-01 version.

At this stage, I have anticipated that I will be using C++ to implement this library. The standards I choose to test for compatibility are C++03, C++11, C++14, C++17, and C++20 (if it becomes official in the lifespan of this library).

I will also be checking the compatibility with the following C++ compilers: gcc (version 5.0\~{}8.0), clang (version 5.0, 6.0), msvc (the versions distributed with vs2017 and vs2019), and icc (16.0\~{}19.0) 

\subsection{Traceability Between Test Cases and Instance Modules}
We show each instance module's covering tests. \wss{You don't have modules.  I
  think you mean requirements?}\an{Actually, I mean instance modules. Each set of tests covers the related requirements to this instance module, and how the requirements apply to this instance module.}

\begin{itemize}
\item IM1: \textit{Test of transformation (even function)} and \textit{Test of
    transformation (odd function)}.
\item IM2: \textit{Test of approximated function value}
\item IM3: \textit{Test of addition}
\item IM4: \textit{Test of subtraction}
\item IM5: \textit{Test of multiplication}
\item IM6: \textit{Test of division}
\item IM7: \textit{Test of function of CFS}
\item IM8: \textit{Test of amplitude}
\item IM9: \textit{Test of tolerated equality}
\item IM10: \textit{Test of conversion from other data format}
\item IM11: \textit{Test of conversion to other data format}
\end{itemize}
				
\wss{We discussed that your unit verification and validation plan will be
  identical to the system plan.  I agree there is overlap, but I expect that you
  will have unique tests for the unit case.  For instance, if you introduce a
  data structure for the CFSs, then you want to be able to check each of the
  methods for the CFSs.}\an{Indeed. The contents you mentioned will be elaborated in UnitVnV.}

\bibliographystyle{plainnat}

\bibliography{SRS}

\end{document}